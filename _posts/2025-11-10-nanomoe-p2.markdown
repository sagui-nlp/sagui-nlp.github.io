---
layout: default
title:  "From NanoGPT To nanoMoE: Part-2"
date:   2025-11-10 09:00:00 -0300
categories: moe
---

# *From NanoGPT to nanoMoE*: Part-2

**Understanding the MoE Layer**

> In Part-1 we started by explaining why starting with a small model is helpful, we went through the parameters that constitutes the GPT-2, how to train it and how the Perplexity loss works. This time we will look into what makes a MoE Layer, how to compute the router loss, MoE loss and how to combine them. We will also into other concepts like capacity and explain how they influence the experts.

---

## What weâ€™re building (TL;DR)

Given a number of experts **n_experts**, the number of desired active experts **k** and a sequence of **N** tokens:

1. The **Router** model will calculate $ score(x_i, e_j) \in \[0,1\] $, that is how likely the expert _j_ will be the best choice for processing the _i-th_ token in the sequence.
1. For each token, select the **top-k** highest-scoring experts.
1. Drop every token from each expert that exceeds the set **capacity**.
1. The dropped tokens go through an **identity** projection.
1. Let every expert **project** the remaining tokens.
1. Calculate the loss using the **importance** and **load** from each expert.

---

## The Expert block

The expert is actually the same as the conventional feedforward network from the Transformers architecture:

```python
class Expert(nn.Module):
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (N_tokens, d_model)
        return self.dropout(self.fc2(self.act(self.fc1(x))))
```

As in the MLP from Transformers, the **d_ff** parameter is usually 4 times bigger than **d_model**.

---

## The MoE layer skeleton

```python
class MoELayer(nn.Module):
   def __init__(self, config):
        super().__init__()
        self.d_model = config.n_embd
        self.n_experts = config.n_experts
        self.capacity_factor = config.capacity_factor  
        self.k = config.k                             
        self.experts_weight = config.experts_weight
        self.router_weight = config.router_weight
        self.dropout = config.dropout

        # ... rest of the code
```

Here we give a better explanation of the parameters of a **MoE Layer**:

- **d_model**: is the hidden size of the model, for instance **BERT** used a hidden size of 768.
- **n_experts**: the maximum number of experts available
- **capacity_factor**: the capacity of each expert is the maximum number of tokens it can process. The final capacity is calculated using the formula:

$$ capacity = \frac{S * \text{capacity_factor}}{\text{n_experts}} $$

Where **S** is the total number of tokens in the batch. If this factor is equal to the number of experts then there is no limit to the number of tokens per expert, this could lead to favoritism, where one expert process most of the tokens. If equal 1 then each expert can only process a equal part as the rest, but it makes more tokens go through the identity projection, decreasing the effect of the experts on the output. A good value is very important to stability in training.

- **k**: the maximum number of experts that can process one token.
- **experts_weight** and **router_weight**: Scaling factors that help control the importance of each part of the loss (discussed more later).

---

## Choosing the top-k experts

Now lets check the code for finding the **top-k** experts for each token. Lets assume that **H** is the matrix of (B, N, d) that is, **B** sequences of length **N** with an embedding dimension of **d**.

```python
## we first flatten the H matrix
H_flat = H.reshape(S, d)               # (S,d)
## then each token is processed by the router 
router_logits = self.gate(H_flat).float()    # (S,E)
## finally apply softmax to get a score between 0 and 1
router_probs = F.softmax(router_logits, -1)  # (S,E)

## we use torch.topk to get scores and the indices of the highest scores from the router_probs
## using the router_logits would also work if we only need the expert indices
topk_scores, topk_experts = torch.topk(router_probs, k=self.k, dim=-1)  # (S,k)
```

---

## Applying capacity limits

As mentioned before, the experts have a total token capacity. Let's say 200 tokens are assigned to the same expert, however, the capacity is 150, that means that 50 tokens will not be processed by this expert. The common fallback applied here is the identity projection, if no other expert is assigned to this token then it will remain unchanged.

To select the tokens that will be processed and the ones that will fallback, we can either assign token to each expert based on the order of appearance in the batch or choose based on their scores. For this implementation we selected the last method. 

```python
## compute the capacity that each expert will have
capacity = self._compute_capacity(S)
per_expert_tokens, per_expert_scores = [], []

for e in range(self.n_experts):
    ## get the tokens and scores that were assigned to the e-th expert
    mask_e = (topk_experts == e)   # (S,k)
    token_ids = mask_e.nonzero(as_tuple=False)[:, 0]  # (N_assign_e,)
    scores_e  = topk_scores[mask_e]                   # (N_assign_e,)

    ## if the total is greater than the capacity
    if scores_e.numel() > capacity:
        ## select the ones with the highest score
        keep_scores, keep_idx = torch.topk(scores_e, k=capacity, dim=0)
        token_ids = token_ids[keep_idx]
        scores_e  = keep_scores

    per_expert_tokens.append(token_ids)
    per_expert_scores.append(scores_e)
```


---

## Computing the experts outputs

```python
## copy the current H_flat matrix
## this is important for the tokens that won't be processed by any expert
H_out_flat = H_flat.clone()

for e in range(self.n_experts):
    ## get the token_ids for the e-th expert
    token_ids = per_expert_tokens[e]
    if token_ids.numel() == 0: 
        continue

    ## get the current representation of the token_ids
    expert_in  = H_flat[token_ids]            # (N_e,d)
    ## compute the expert projection
    expert_out = self.experts[e](expert_in)   # (N_e,d)
    ## add the expert projection to the existing embedding
    H_out_flat.index_add_(0, token_ids, expert_out)

## reshape back to (B, N, d)
H_out = H_out_flat.view(B, N, d)
```

If a token is not processed by any expert its representation will remain unchanged. The other tokens will have its new representation being a sum of the initial representation plus the calculated projections from each expert. If needed, we can also weight every expert projection by the score or importance of the token being processed.


---

## Router and Balance loss

We do not want that any expert gets a preference, by being activated too often, or be ignored by not being assigned to any token.
To counter these possible effects we introduce two components to the loss.

The first is the **Router loss** as given by equation:

$$ r * \frac{ \log \left( \sum^S \exp(x) \right)^2 }{S}$$

Where **x** are the logits calculated by the router and **r** is a weighing coefficient to help control the effect of the result on the total loss.

The other part is the **Balance loss**:

$$ w * \text{n_experts} * \frac{\text{importance} * \text{load}}{S} $$

The **importance** is the mean probability assigned to each expert. 
The **load** is number of tokens assigned to each expert.

The **Balance loss** penalizes both the extremities:

 - Experts that are too important for a few tokens (small load)
 - Experts that have too much low score tokens assigned to it (small importance)

The ideal result is a well balanced set of experts, where each expert is important to a equal number of tokens.

Just like for the **Router loss**, **w** is a coefficient to help control the contribution of the **Balance loss**.

```python

importance = router_probs.mean(dim=0)  # (E,)
load = torch.zeros(self.n_experts, device=H.device, dtype=router_probs.dtype)
for e in range(self.n_experts):
    load[e] = per_expert_tokens[e].numel()

router_loss = (
    self.r * (torch.logsumexp(router_logits, dim=-1) ** 2.0).mean()
)
balance_loss = (self.n_experts * (importance * load).mean()) * self.w
moe_loss = balance_loss.to(H.dtype) + router_loss.to(H.dtype)
```

---

## Debug Checklist

**MoE** gives new powers to the Transformers architecture, however it also adds many new hyperparameters. Here are somethings to track to make sure the training is going well:

* **Number of dropped tokens**: If too many tokens are being dropped that means the capacity is too low and should be increased as it gives the model more expression capability.
* **Stability of Experts**: For a validation batch you can keep track of the experts chosen for each token, if the scores and the experts change too much that means that the training is unstable. This behaviour should decrease the longer we train. If it does not than we either check for bugs in our code or try to adjust the **r** and **w** coefficients.
* **Loss stability**: The stability of the loss should also be tracked a too unstable loss can point to bugs or that other parameters like number of experts, top-k and capacity need to be fine tuned.

---

## Next Steps

In the next post we will integrate the **MoE** into the training loop and check how hyperparameters affect performance.

